{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cc9236-b317-4433-b32e-05d4db3bec69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:08<00:00,  2.04s/it]\n",
      "Device set to use cuda:0\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset    | 10/7019 [04:57<56:40:19, 29.11s/it]\n",
      "Verbalizing Urban Data from census:   0%|▏                                                                      | 13/7019 [06:47<61:02:39, 31.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 202\u001b[0m\n\u001b[1;32m    194\u001b[0m     verbalizator\u001b[38;5;241m.\u001b[39mverbalize_dataset(path_input \u001b[38;5;241m+\u001b[39m in_file_name, out_file_name, type_approach)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m#if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#    parser = argparse.ArgumentParser()\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m#    parser.add_argument(\"--verbalization_level\", type=str, required=True)\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m#    parser.add_argument(\"--type_approach\", type=str, required=True)\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m#    args = parser.parse_args()\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#    main(args)\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 194\u001b[0m, in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m     out_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_shot_verbalized_urban_data_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m verbalization_level \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m verbalizator \u001b[38;5;241m=\u001b[39m UrbanDataVerbalizer(batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m, verbalization_level \u001b[38;5;241m=\u001b[39m verbalization_level, output_dir \u001b[38;5;241m=\u001b[39m path_output)\n\u001b[0;32m--> 194\u001b[0m \u001b[43mverbalizator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbalize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43min_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_file_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtype_approach\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m, in \u001b[0;36mUrbanDataVerbalizer.verbalize_dataset\u001b[0;34m(self, input_csv, output_file, type_approach)\u001b[0m\n\u001b[1;32m    153\u001b[0m batch \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Generate narrative\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     narrative \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_comprehensive_narrative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfield_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Write to output file with metadata\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbalization_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcensus\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 106\u001b[0m, in \u001b[0;36mUrbanDataVerbalizer.generate_comprehensive_narrative\u001b[0;34m(self, row, field_description, example)\u001b[0m\n\u001b[1;32m    104\u001b[0m stop_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Generate narrative with comprehensive settings\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased sequence length\u001b[39;49;00m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicitly activate truncation\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\n\u001b[1;32m    117\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Extract and combine context with generated text\u001b[39;00m\n\u001b[1;32m    120\u001b[0m narrative \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(prompt, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1361\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1362\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         )\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1375\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1374\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1375\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msequences\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/generation/utils.py:3200\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3197\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[1;32m   3199\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[1;32m   3202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   3203\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3204\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3206\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/llmdata/home/hjarquin/deep_nlp/lib/python3.10/site-packages/transformers/generation/utils.py:2401\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[0;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   2400\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 2401\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "class UrbanDataVerbalizer:\n",
    "    def __init__(self,\n",
    "                 model_name = \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "                 output_dir = \"verbalization_results\",\n",
    "                 batch_size = 50,\n",
    "                verbalization_level = \"census\"):\n",
    "        \"\"\"\n",
    "        Initialize the verbalization pipeline with enhanced configuration\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.output_dir = output_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.verbalization_level = verbalization_level\n",
    "\n",
    "        # Increased sequence length for comprehensive narratives\n",
    "        self.max_length = 1024  # Increased from previous implementation (2048)\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "        # Updated generation pipeline with explicit truncation\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def prepare_row_context(self, row):\n",
    "        \"\"\"\n",
    "        Prepare contextual identifiers for the row, based on the verbalization level\n",
    "\n",
    "        Args:\n",
    "            row (pd.Series): DataFrame row\n",
    "\n",
    "        Returns:\n",
    "            str: Formatted context string\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.verbalization_level == \"zone\":\n",
    "            return (f\"Year {row['year']}, Statistical Zone {row['zone_stat']} ({row['desc_zone']}), \"\n",
    "                    f\"District {row['district']}: \")\n",
    "        elif self.verbalization_level == \"district\":\n",
    "            return (f\"Year {row['year']}, District {row['district']}: \")\n",
    "        elif self.verbalization_level == \"census\":\n",
    "            return (f\"Year {row['year']}, Census Area {row['cens']}, \"\n",
    "                    f\"Statistical Zone {row['zone_stat']} ({row['desc_zone']}), \"\n",
    "                    f\"District {row['district']}: \")\n",
    "\n",
    "    def generate_comprehensive_narrative(self, row, field_description, example = None):\n",
    "        \"\"\"\n",
    "        Generate a single-paragraph comprehensive narrative\n",
    "\n",
    "        Args:\n",
    "            row (pd.Series): DataFrame row with urban data\n",
    "\n",
    "        Returns:\n",
    "            str: Comprehensive verbalization of the row\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Prepare row context and facts\n",
    "            row_context = self.prepare_row_context(row)\n",
    "            facts = {key: row.get(key, 'Not available') for key in row.index}\n",
    "\n",
    "            # Construct detailed prompt for single-paragraph narrative\n",
    "            prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert urban data analyst. Convert census and transport data into clear narratives.\n",
    "<|start_header_id|>user<|end_header_id|>\\n\n",
    "Generate a comprehensive, single-paragraph narrative about an urban area based on the following numeric data. \n",
    "The narrative must:\n",
    "- Be concise, informative, cover all key aspects of the urban landscape, and limit to a single paragraph.\n",
    "- Include and reflect the exact values as given in the Numeric Facts, without modification or approximation.\n",
    "- Focus solely on describing the attributes defined in Field Descriptions, matching each field with its corresponding value.\n",
    "- Avoid drawing conclusions, making assumptions, or interpreting the significance of the data.\n",
    "- Avoid comparing the data to other entries, past values, or the example provided.\n",
    "\n",
    "Unique Identifier: {row_context}\n",
    "Field Descriptions: {field_description}\"\"\"\n",
    "            if example is not None:\n",
    "                prompt += f\"\"\"\n",
    "Example:\n",
    "{example}\"\"\"\n",
    "            prompt += f\"\"\"\n",
    "Numeric Facts: {json.dumps(facts)}\n",
    "Generated Narrative: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\"\"\n",
    "            \n",
    "            stop_token_id = self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "            # Generate narrative with comprehensive settings\n",
    "            response = self.generator(\n",
    "                prompt,\n",
    "                max_new_tokens=self.max_length,  # Increased sequence length\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.6,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                truncation=True,  # Explicitly activate truncation\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=stop_token_id,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "\n",
    "            # Extract and combine context with generated text\n",
    "            narrative = response[0]['generated_text'].replace(prompt, '').strip()\n",
    "            return row_context + narrative\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating narrative: {e}\")\n",
    "            return f\"Error processing row: {str(e)}\"\n",
    "\n",
    "    def verbalize_dataset(self, input_csv, output_file = 'verbalized_urban_data.txt', type_approach = \"zero-shot\"):\n",
    "        \"\"\"\n",
    "        Verbalize entire dataset with progress tracking\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(input_csv)\n",
    "        total_rows = len(df)\n",
    "\n",
    "        output_path = os.path.join(self.output_dir, output_file)\n",
    "\n",
    "        with open('../data/input/description_' + self.verbalization_level + '.json', 'r') as file:\n",
    "            field_description = json.load(file)\n",
    "\n",
    "        if type_approach == \"few-shot\":\n",
    "            with open('../data/input/sample_' + self.verbalization_level + '.txt', 'r') as file:\n",
    "                example = file.read()\n",
    "        else:\n",
    "            example = None\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(f\"# Urban Data Verbalization Results ({type_approach})\\n\")\n",
    "            outfile.write(f\"# Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            outfile.write(f\"# Source file: {input_csv}\\n\")\n",
    "            outfile.write(f\"# Total records: {total_rows}\\n\\n\")\n",
    "            \n",
    "            with tqdm(total=total_rows, desc=\"Verbalizing Urban Data from \" + self.verbalization_level) as pbar:\n",
    "                for i in range(0, total_rows, self.batch_size):\n",
    "                    batch = df.iloc[i:i+self.batch_size]\n",
    "\n",
    "                    for _, row in batch.iterrows():\n",
    "                        # Generate narrative\n",
    "                        narrative = self.generate_comprehensive_narrative(row, field_description, example)\n",
    "\n",
    "                        # Write to output file with metadata\n",
    "                        if self.verbalization_level == \"census\":\n",
    "                            record_id = f\"Census {row['cens']}, Zone {row['zone_stat']}, District {row['district']}, Year {row['year']}\"\n",
    "                        elif self.verbalization_level == \"zone\":\n",
    "                            record_id = f\"Zone {row['zone_stat']}, District {row['district']}, Year {row['year']}\"\n",
    "                        elif self.verbalization_level == \"district\":\n",
    "                            record_id = f\"District {row['district']}, Year {row['year']}\"\n",
    "                        \n",
    "                        outfile.write(f\"## {record_id}\\n\\n\")\n",
    "                        outfile.write(narrative + \"\\n\\n\")\n",
    "                        outfile.write(\"-\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                        # Update progress bar\n",
    "                        pbar.update(1)\n",
    "\n",
    "        print(f\"Verbalization complete. Results saved to {output_path}\")\n",
    "\n",
    "# Usage Example\n",
    "def main(args):\n",
    "    path_input           = \"../data/input/\"\n",
    "    path_output          = \"../verbalization_results/\"\n",
    "    verbalization_level  = args.verbalization_level # census, zone, district\n",
    "    type_approach        = args.type_approach # \"few-shot\", \"zero-shot\"\n",
    "    \n",
    "    if verbalization_level == \"zone\" or verbalization_level == \"district\":\n",
    "        in_file_name = \"population-and-transport-\" +  verbalization_level + \".csv\"\n",
    "    else:\n",
    "        in_file_name = \"population-and-transport.csv\"\n",
    "\n",
    "    if type_approach == \"few-shot\":\n",
    "        out_file_name = \"few_shot_verbalized_urban_data_\" + verbalization_level + \".txt\"\n",
    "    elif type_approach == \"zero-shot\":\n",
    "        out_file_name = \"zero_shot_verbalized_urban_data_\" + verbalization_level + \".txt\"\n",
    "    \n",
    "    verbalizator = UrbanDataVerbalizer(batch_size = 50, verbalization_level = verbalization_level, output_dir = path_output)\n",
    "    verbalizator.verbalize_dataset(path_input + in_file_name, out_file_name, type_approach)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--verbalization_level\", type=str, required=True)\n",
    "    parser.add_argument(\"--type_approach\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
